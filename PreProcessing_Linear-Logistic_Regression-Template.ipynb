{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# axis = 0 => column & 1 = row\n",
    "# y_true = real value also called as ground truth\n",
    "# y^ = predicted value\n",
    "# 4 types of algo - Classification, Regression, Clustering and Rule-based detection - KNN can do 1st 3\n",
    "# performance measure - in Classification = accuracy, Regression = residual variance\n",
    "# if the transformation is linear we do not need activation function\n",
    "# np.reshape(-1,1) -> gives you list of list or we can call it arrays of array\n",
    "\n",
    "##################################################################################################################\n",
    "# TENSORFLOW IMPLEMENTATION DETAILS\n",
    "# operation of single neuron - linear transformation (wx+b) input fed to activation function = actvfun(wx+b)\n",
    "# objective is to find best w and b - how? => i/p data, cost function and optimizer(to minimize loss function)\n",
    "#\n",
    "# method is as follows for Linear Regression\n",
    "# - Affine Transformation -> specify cost function(MSE) -> Minimize cost function using SGD -> then train\n",
    "# \n",
    "# 1 datapoint = SGD, batches = mini batch SGD\n",
    "# \n",
    "#\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "# FIT and FIT_TRANSFORM\n",
    "#\n",
    "# always fit_tranform training data but only transform x-test and x-cv data\n",
    "# you can scale the dummy variables if you want to on x data\n",
    "# in Logistic Regression - we fit on x and y data to learn the correlation\n",
    "# then predict using this correlation with test set\n",
    "# \n",
    "#\n",
    "#\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "# FEATURE SCALING\n",
    "#\n",
    "# feature scaling helps to converge much faster\n",
    "# y is categorical variable so we dont need to scale those as it take few values\n",
    "# but for regression model y will take huge values so we can scale in that\n",
    "# feature scaling when we have 2 features (f1* f2) with different scale \n",
    "# e.g f1 = x coordinates and f2 = y coordinates\n",
    "# how to measure distance - \n",
    "# (L2 norm)euclidean dist between 2 points p1(x1,y1) & p2(x2,y2) = sqrt(square(x2-x1) - sqaure(y2-y1)) \n",
    "# (L1 norm)manhattan dist between 2 points p1(x1,y1) & p2(x2,y2) = ||x1-y1|| ||x2-y2||\n",
    "# (p norm)minkowski dist between 2 points p1(x1,y1) & p2(x2,y2) = (||x1-y1||p)**p\n",
    "# if p = 1 L1 norm and p=2 then L2 norm\n",
    "# feature scaling types - standardization and normalization\n",
    "# column normalization   - (x-min(x))/(max(x)-min(x)) - not used much\n",
    "# column standardization (called mean centring) - (x-mean(x))/(std-dev(x)) - this is more often used\n",
    "# since it has nice relation with Gausian Distribution. \n",
    "# the col can come in any distribution - after col distibution converted to ~ N (0,1)\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "# Confusion Matrix - shows the correct prediction made by the model and incorrect one\n",
    "#  [ 84, 3\n",
    "#    1 , 24 ]\n",
    "# reading CM - 84+24 are the correct predictions and 1+3 are incorrect ones\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#missing data - imputer encode categorical data & onehot for dummy variables\n",
    "# Standarize the data as well\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "# for Logistic Regression and Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "# for train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "#tensorflow for TF implementation\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it the csv file then read using pandas\n",
    "# e.g. there will be 3 cols \n",
    "df = pd.read_csv('filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file will have last col as dependent variable\n",
    "# x will contain all the col except the last one\n",
    "# take all the cols except the last one (dependent)\n",
    "# this is justone way we extract, I will keep adding\n",
    "# other ways here later\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data , Categorical , OneHot Encoding, Train-Test Split  Feature Sclaing - To build a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create a function - pass x and y variable and o/p will be the values, allow \n",
    "# to pass command as Imputer, LabelEncoder or OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the imputer with mean and fit it on x this fit is only on missing cols\n",
    "# once you fit on those missing cols then you run the tranform to put the mean values \n",
    "# on missing cols\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imputer.fit(x[:, 1:3])\n",
    "x[:, 1:3] = imputer.transformx[:, 1:3]\n",
    "\n",
    "# encoding categorial data - convert  text to numbers\n",
    "# imagine categorial col is 1st col and it has 2 countries\n",
    "\n",
    "labelEncoder_x = LabelEncoder()\n",
    "x[:,0] = labelEncoder.fit_transform(x[:,0])\n",
    "labelEncoder_y = LabelEncoder()\n",
    "y = labelEncoder_y.fit_transform(y)\n",
    "\n",
    "# once we have done this then this col would have \n",
    "# 0 as Singapore and 1 as India - for e.g.\n",
    "# now 0 < 1 but this in not correct for country\n",
    "# so use dummy variable concept\n",
    "# it will create 2 dummy variables - i.e. 2 categorial data will have 2^2 - is this a good \n",
    "# encode for the first col only\n",
    "onehotEncoder = OneHotEncoder(categorial_variable=[0])\n",
    "# since I have already mentioned the variable at 0\n",
    "x = onehotEncoder.fit(x).toarray()\n",
    "\n",
    "\n",
    "# train, test split, we can add x_cv anf y_cv later. test_size can vary\n",
    "# random_state can be any number - if required\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# feature scaling\n",
    "\n",
    "std = StandardScaler()\n",
    "x = std.fit_transform(x_train)\n",
    "y = std.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification technique\n",
    "# probablity or likelyhood of that person taking an offer\n",
    "# always it will be between 0 and 1\n",
    "# it still fits the line but a curve rather than straight line\n",
    "\n",
    "# so consider y = w0x + w1 and we apply a sigmoid function \n",
    "# p = 1/(1+e(-y)) [i.e. e power -y]\n",
    "# put the values of y from y = wx + b on the sigmoid function\n",
    "# \n",
    "\n",
    "# formula for logistic regression - ln(p/(1-p)) = w0x + w1 \n",
    "# using this formula, x axis will have data points and y axis contains predicted probablities (p^)\n",
    "# \n",
    "\n",
    "# LR is a straight line if the classifier is a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "# fit the training data\n",
    "logReg.fit(x_train, y_train)\n",
    "# predict the data on test data\n",
    "y_pred = logReg.predict(x_test)\n",
    "\n",
    "# print the coef and intercept term\n",
    "print(logReg.coef_)\n",
    "print(logReg.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix - Build a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of model using Confusion Matrix\n",
    "# Confusion Matrix - shows the correct prediction made by the model and incorrect one\n",
    "# we will build a function\n",
    "# y_pred is what predicted by the Logistic Regression\n",
    "cm = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same method to buid the placeholders and variables like in LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function call for Logistic regression to train\n",
    "# this optimizer can be anything\n",
    "run_training(5000, optimizer, len(x_data))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression - y = mx + b -> single independent variable\n",
    "# multiple linear regression - y = mx + w0x0 + w1x1 + ... +wnxn -> multiple independent variable\n",
    "\n",
    "# y = mx+b which - if line passes through origin then b = 0 (y intercept)\n",
    "# usually it is also written as y = wx + b\n",
    "# y is the actual value and y^ (predicted) is the point on which the line passes\n",
    "\n",
    "# Ordinary Least Square(OLS) method to find the best fit line which passes thru all points\n",
    "# ols = sum[sqr(y - y^)] \n",
    "# minimize the OLS to find the best fit\n",
    "# Linear least square (error) (LLS) - (y-y^)\n",
    "\n",
    "# this is same as Logistic Regression just that the classifier will change to LinearRegression()\n",
    "\n",
    "\n",
    "linReg = LinearRegression()\n",
    "# fit the training data\n",
    "linReg.fit(x_train, y_train)\n",
    "\n",
    "# print the coef and intercept term\n",
    "print(\"after training\\n\")\n",
    "print(\"intercept\", linReg.intercept_)\n",
    "print(\"coeff\", linReg.coef_)\n",
    "\n",
    "# predict the data on test data which it has ever seen before\n",
    "y_pred = linReg.predict(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Mean Absolute Error (MAE) $$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "Mean Squared Error (MSE) $$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "Root Mean Squared Error (RMSE) $$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where y_test will have the actual value - we can check this with scatter plot\n",
    "plt.scatter(y_test, y_pred) # if o/p is a straight line then pred is correct\n",
    "\n",
    "# check the distribution \n",
    "sns.display((y_test-y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_pred)\n",
    "metrics.mean_squared_error(y_test, y_pred)\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Implementation - Simple & with Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start off with some random value\n",
    "w = tf.Variable(tf.zeros([1,1]), name=\"w\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"b\")\n",
    "\n",
    "# placeholder None because we dont know how many data points and \n",
    "# 1 because each xdata & ydata is 1 point only - x& y are from our data\n",
    "# y is the actual data from the dataset\n",
    "x = tf.placeholder(tf.float32, [None, 1], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, 1], name =\"y\")\n",
    "wx = matmul(x,w)\n",
    "\n",
    "# o/p of single neuro - this y is predicted value (y^) i.e y_ here\n",
    "y_ = wx + b \n",
    "\n",
    "# cost function or loss - objective is minimizing the loss - using MSE\n",
    "cost = tf.reduce_mean(tf.square(y - y_))\n",
    "\n",
    "# using optimizer we can use SGD as well\n",
    "# 1 here is the learning rate and objective is minimize cost function\n",
    "# use either fltr or GradientDescent\n",
    "optimizer = tf.train.FtlrOptimizer(1).minimize(cost)\n",
    "#optimizer = tf.train.GradientDesecentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass training steps, optimizer to use and batch size\n",
    "x_data_size = len(x_train)\n",
    "\n",
    "def run_training(steps, optimizer, batch_size):\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(steps):\n",
    "      if x_data_size == batch_size:\n",
    "        batch_start_idx = 0\n",
    "      elif dataset_size < batch_size:\n",
    "        raise ValueError(\"dataset_size: %d, must be greater than batch_size: %d\" % (dataset_size, batch_size))\n",
    "      else:\n",
    "        batch_start_idx = (i * batch_size) % (x_data_size)\n",
    "\n",
    "        batch_end_idx = batch_start_idx + batch_size\n",
    "\n",
    "      # Access the x and y values in batches\n",
    "        batch_xs = x_train[batch_start_idx : batch_end_idx]\n",
    "        batch_ys = y_train[batch_start_idx : batch_end_idx]\n",
    "\n",
    "      # Reshape the 1-D arrays as 2D feature vectors with many rows and 1 column\n",
    "      # incase we need to reshape or else we can directly feed to the dict\n",
    "        feed = { x: batch_xs.reshape(-1, 1), y_: batch_ys.reshape(-1, 1) }\n",
    "\n",
    "        sess.run(train_step, feed_dict=feed)\n",
    "\n",
    "      # Print result to screen for every 500 iterations\n",
    "      if (i + 1) % 500 == 0:\n",
    "        print(\"After %d iteration:\" % i)\n",
    "        print(\"W: %f\" % sess.run(w))\n",
    "        print(\"b: %f\" % sess.run(b))\n",
    "\n",
    "        print(\"cost: %f\" % sess.run(cost, feed_dict=feed))\n",
    "\n",
    "#to call the function        \n",
    "run_training(5000, optimizer, len(x_data))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
